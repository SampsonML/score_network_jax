{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A jax implementation of the diffusion model from \n",
        "the paper \"Improved Techniques for Training Score-Based Generative Models\"\n",
        "https://arxiv.org/abs/2006.09011\n",
        "Code taken primarily from https://github.com/yang-song/score_sde/\n",
        "Modifications by Matt Sampson include:\n",
        "    - Minor updates to use the latest version of flax\n",
        "\"\"\"\n",
        "\n",
        "%matplotlib inline\n",
        "import functools\n",
        "import math\n",
        "import string\n",
        "from typing import Any, Sequence, Optional\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.nn as jnn\n",
        "import jax.numpy as jnp\n",
        "import jax.nn.initializers as init\n",
        "import matplotlib as mpl\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import flax\n",
        "Path(\"outputs\").mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Common layers for defining score networks.\n",
        "\"\"\"\n",
        "class InstanceNorm2dPlus(nn.Module):\n",
        "  \"\"\"InstanceNorm++ as proposed in the original NCSN paper.\"\"\"\n",
        "  bias: bool = True\n",
        "\n",
        "  @staticmethod\n",
        "  def scale_init(key, shape, dtype=jnp.float32):\n",
        "    normal_init = init.normal(0.02)\n",
        "    return normal_init(key, shape, dtype=dtype) + 1.\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    means = jnp.mean(x, axis=(1, 2))\n",
        "    m = jnp.mean(means, axis=-1, keepdims=True)\n",
        "    v = jnp.var(means, axis=-1, keepdims=True)\n",
        "    means_plus = (means - m) / jnp.sqrt(v + 1e-5)\n",
        "\n",
        "    h = (x - means[:, None, None, :]) / jnp.sqrt(jnp.var(x, axis=(1, 2), keepdims=True) + 1e-5)\n",
        "\n",
        "    h = h + means_plus[:, None, None, :] * self.param('alpha', InstanceNorm2dPlus.scale_init, (1, 1, 1, x.shape[-1]))\n",
        "    h = h * self.param('gamma', InstanceNorm2dPlus.scale_init, (1, 1, 1, x.shape[-1]))\n",
        "    if self.bias:\n",
        "      h = h + self.param('beta', init.zeros, (1, 1, 1, x.shape[-1]))\n",
        "\n",
        "    return h\n",
        "\n",
        "\n",
        "def ncsn_conv1x1(x, out_planes, stride=1, bias=True, dilation=1, init_scale=1.):\n",
        "  \"\"\"1x1 convolution with PyTorch initialization. Same as NCSNv1/v2.\"\"\"\n",
        "  init_scale = 1e-10 if init_scale == 0 else init_scale\n",
        "  kernel_init = jnn.initializers.variance_scaling(1 / 3 * init_scale, 'fan_in',\n",
        "                                                  'uniform')\n",
        "  kernel_shape = (1, 1) + (x.shape[-1], out_planes)\n",
        "  bias_init = lambda key, shape: kernel_init(key, kernel_shape)[0, 0, 0, :]\n",
        "  output = nn.Conv(out_planes, kernel_size=(1, 1),\n",
        "                   strides=(stride, stride), padding='SAME', use_bias=bias,\n",
        "                   kernel_dilation=(dilation, dilation),\n",
        "                   kernel_init=kernel_init,\n",
        "                   bias_init=bias_init)(x)\n",
        "  return output\n",
        "\n",
        "\"\"\" Old version of ncsn_conv3x3 \"\"\"\n",
        "\"\"\"\n",
        "def ncsn_conv3x3(x, out_planes, stride=1, bias=True, dilation=1, init_scale=1.):\n",
        "  # 3x3 convolution with PyTorch initialization. Same as NCSNv1/NCSNv2.\n",
        "  init_scale = 1e-10 if init_scale == 0 else init_scale\n",
        "  kernel_init = jnn.initializers.variance_scaling(1 / 3 * init_scale, 'fan_in',\n",
        "                                                  'uniform')\n",
        "  kernel_shape = (3, 3) + (x.shape[-1], out_planes)\n",
        "  bias_init = lambda key, shape: kernel_init(key, kernel_shape)[0, 0, 0, :]\n",
        "  output = nn.Conv(out_planes,\n",
        "                   kernel_size=(3, 3),\n",
        "                   strides=(stride, stride),\n",
        "                   padding='SAME',\n",
        "                   use_bias=bias,\n",
        "                   kernel_dilation=(dilation, dilation),\n",
        "                   kernel_init=kernel_init,\n",
        "                   bias_init=bias_init)(x)\n",
        "  return output\n",
        "\"\"\"\n",
        "\n",
        "def ncsn_conv3x3(x, out_planes, stride=1, bias=True, dilation=1, init_scale=1.):\n",
        "  \"\"\"3x3 convolution with PyTorch initialization. Same as NCSNv1/NCSNv2.\"\"\"\n",
        "  init_scale = 1e-10 if init_scale == 0 else init_scale\n",
        "  kernel_init = jnn.initializers.variance_scaling(1 / 3 * init_scale, 'fan_in',\n",
        "                                                  'uniform')\n",
        "  kernel_shape = (3, 3) + (x.shape[-1], out_planes)\n",
        "  bias_init = lambda key, shape: kernel_init(key, kernel_shape)[0, 0, 0, :]\n",
        "  output = nn.Conv(out_planes,\n",
        "                   kernel_size=(3, 3),\n",
        "                   strides=(stride, stride),\n",
        "                   padding='SAME',\n",
        "                   use_bias= False, #bias,\n",
        "                   kernel_dilation=(dilation, dilation),\n",
        "                   kernel_init=kernel_init)(x) #,\n",
        "                   #bias_init=bias_init)(x)\n",
        "  return output\n",
        "\n",
        "# How to call bias_init search this and see - flax outdated thing\n",
        "# Make a github repo for this\n",
        "# Get much more familiar with github\n",
        "# add a requirement.txt to show which versions of modules are needed\n",
        "\n",
        "# ---------------------------------------------------------------- #\n",
        "# Functions below are ported over from the NCSNv1/NCSNv2 codebase: #\n",
        "# https://github.com/ermongroup/ncsn                               #\n",
        "# https://github.com/ermongroup/ncsnv2                             #\n",
        "# ---------------------------------------------------------------- #\n",
        "\n",
        "class CRPBlock(nn.Module):\n",
        "  \"\"\"CRPBlock for RefineNet. Used in NCSNv2.\"\"\"\n",
        "  features: int\n",
        "  n_stages: int\n",
        "  act: Any = nn.relu\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = self.act(x)\n",
        "    path = x\n",
        "    for _ in range(self.n_stages):\n",
        "      path = nn.max_pool(\n",
        "        path, window_shape=(5, 5), strides=(1, 1), padding='SAME')\n",
        "      path = ncsn_conv3x3(path, self.features, stride=1, bias=False)\n",
        "      x = path + x\n",
        "    return x\n",
        "\n",
        "\n",
        "class RCUBlock(nn.Module):\n",
        "  \"\"\"RCUBlock for RefineNet. Used in NCSNv2.\"\"\"\n",
        "  features: int\n",
        "  n_blocks: int\n",
        "  n_stages: int\n",
        "  act: Any = nn.relu\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    for _ in range(self.n_blocks):\n",
        "      residual = x\n",
        "      for _ in range(self.n_stages):\n",
        "        x = self.act(x)\n",
        "        x = ncsn_conv3x3(x, self.features, stride=1, bias=False)\n",
        "      x = x + residual\n",
        "\n",
        "    return x\n",
        "\n",
        "class MSFBlock(nn.Module):\n",
        "  \"\"\"MSFBlock for RefineNet. Used in NCSNv2.\"\"\"\n",
        "  shape: Sequence[int]\n",
        "  features: int\n",
        "  interpolation: str = 'bilinear'\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, xs):\n",
        "    sums = jnp.zeros((xs[0].shape[0], *self.shape, self.features))\n",
        "    for i in range(len(xs)):\n",
        "      h = ncsn_conv3x3(xs[i], self.features, stride=1, bias=True)\n",
        "      if self.interpolation == 'bilinear':\n",
        "        h = jax.image.resize(h, (h.shape[0], *self.shape, h.shape[-1]), 'bilinear')\n",
        "      elif self.interpolation == 'nearest_neighbor':\n",
        "        h = jax.image.resize(h, (h.shape[0], *self.shape, h.shape[-1]), 'nearest')\n",
        "      else:\n",
        "        raise ValueError(f'Interpolation {self.interpolation} does not exist!')\n",
        "      sums = sums + h\n",
        "    return sums\n",
        "\n",
        "class RefineBlock(nn.Module):\n",
        "  \"\"\"RefineBlock for building NCSNv2 RefineNet.\"\"\"\n",
        "  output_shape: Sequence[int]\n",
        "  features: int\n",
        "  act: Any = nn.relu\n",
        "  interpolation: str = 'bilinear'\n",
        "  start: bool = False\n",
        "  end: bool = False\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, xs):\n",
        "    rcu_block = functools.partial(RCUBlock, n_blocks=2, n_stages=2, act=self.act)\n",
        "    rcu_block_output = functools.partial(RCUBlock,\n",
        "                                        features=self.features,\n",
        "                                        n_blocks=3 if self.end else 1,\n",
        "                                        n_stages=2,\n",
        "                                        act=self.act)\n",
        "    hs = []\n",
        "    for i in range(len(xs)):\n",
        "      h = rcu_block(features=xs[i].shape[-1])(xs[i])\n",
        "      hs.append(h)\n",
        "\n",
        "    if not self.start:\n",
        "      msf = functools.partial(MSFBlock, features=self.features, interpolation=self.interpolation)\n",
        "      h = msf(shape=self.output_shape)(hs)\n",
        "    else:\n",
        "      h = hs[0]\n",
        "\n",
        "    crp = functools.partial(CRPBlock, features=self.features, n_stages=2, act=self.act)\n",
        "    h = crp()(h)\n",
        "    h = rcu_block_output()(h)\n",
        "    return h\n",
        "\n",
        "class ConvMeanPool(nn.Module):\n",
        "  \"\"\"ConvMeanPool for building the ResNet backbone.\"\"\"\n",
        "  output_dim: int\n",
        "  kernel_size: int = 3\n",
        "  biases: bool = True\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    output = nn.Conv(features=self.output_dim,\n",
        "                    kernel_size=(self.kernel_size, self.kernel_size),\n",
        "                    strides=(1, 1),\n",
        "                    padding='SAME',\n",
        "                    use_bias=self.biases)(inputs)\n",
        "    output = sum([\n",
        "      output[:, ::2, ::2, :], output[:, 1::2, ::2, :],\n",
        "      output[:, ::2, 1::2, :], output[:, 1::2, 1::2, :]\n",
        "    ]) / 4.\n",
        "    return output\n",
        "\n",
        "\n",
        "class MeanPoolConv(nn.Module):\n",
        "  \"\"\"MeanPoolConv for building the ResNet backbone.\"\"\"\n",
        "  output_dim: int\n",
        "  kernel_size: int = 3\n",
        "  biases: bool = True\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    output = inputs\n",
        "    output = sum([\n",
        "      output[:, ::2, ::2, :], output[:, 1::2, ::2, :],\n",
        "      output[:, ::2, 1::2, :], output[:, 1::2, 1::2, :]\n",
        "    ]) / 4.\n",
        "    output = nn.Conv(\n",
        "      features=self.output_dim,\n",
        "      kernel_size=(self.kernel_size, self.kernel_size),\n",
        "      strides=(1, 1),\n",
        "      padding='SAME',\n",
        "      use_bias=self.biases)(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  \"\"\"The residual block for defining the ResNet backbone. Used in NCSNv2.\"\"\"\n",
        "  output_dim: int\n",
        "  normalization: Any\n",
        "  resample: Optional[str] = None\n",
        "  act: Any = nn.elu\n",
        "  dilation: int = 1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    h = self.normalization()(x)\n",
        "    h = self.act(h)\n",
        "    if self.resample == 'down':\n",
        "      h = ncsn_conv3x3(h, h.shape[-1], dilation=self.dilation)\n",
        "      h = self.normalization()(h)\n",
        "      h = self.act(h)\n",
        "      if self.dilation > 1:\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "        shortcut = ncsn_conv3x3(x, self.output_dim, dilation=self.dilation)\n",
        "      else:\n",
        "        h = ConvMeanPool(output_dim=self.output_dim)(h)\n",
        "        shortcut = ConvMeanPool(output_dim=self.output_dim, kernel_size=1)(x)\n",
        "    elif self.resample is None:\n",
        "      if self.dilation > 1:\n",
        "        if self.output_dim == x.shape[-1]:\n",
        "          shortcut = x\n",
        "        else:\n",
        "          shortcut = ncsn_conv3x3(x, self.output_dim, dilation=self.dilation)\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "        h = self.normalization()(h)\n",
        "        h = self.act(h)\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "      else:\n",
        "        if self.output_dim == x.shape[-1]:\n",
        "          shortcut = x\n",
        "        else:\n",
        "          shortcut = ncsn_conv1x1(x, self.output_dim)\n",
        "        h = ncsn_conv3x3(h, self.output_dim)\n",
        "        h = self.normalization()(h)\n",
        "        h = self.act(h)\n",
        "        h = ncsn_conv3x3(h, self.output_dim)\n",
        "\n",
        "    return h + shortcut\n",
        "\n",
        "class ConditionalResidualBlock(nn.Module):\n",
        "  \"\"\"The noise-conditional residual block for building NCSNv1.\"\"\"\n",
        "  output_dim: int\n",
        "  normalization: Any\n",
        "  resample: Optional[str] = None\n",
        "  act: Any = nn.elu\n",
        "  dilation: int = 1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, y):\n",
        "    h = self.normalization()(x, y)\n",
        "    h = self.act(h)\n",
        "    if self.resample == 'down':\n",
        "      h = ncsn_conv3x3(h, h.shape[-1], dilation=self.dilation)\n",
        "      h = self.normalization(h, y)\n",
        "      h = self.act(h)\n",
        "      if self.dilation > 1:\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "        shortcut = ncsn_conv3x3(x, self.output_dim, dilation=self.dilation)\n",
        "      else:\n",
        "        h = ConvMeanPool(output_dim=self.output_dim)(h)\n",
        "        shortcut = ConvMeanPool(output_dim=self.output_dim, kernel_size=1)(x)\n",
        "    elif self.resample is None:\n",
        "      if self.dilation > 1:\n",
        "        if self.output_dim == x.shape[-1]:\n",
        "          shortcut = x\n",
        "        else:\n",
        "          shortcut = ncsn_conv3x3(x, self.output_dim, dilation=self.dilation)\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "        h = self.normalization()(h, y)\n",
        "        h = self.act(h)\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "      else:\n",
        "        if self.output_dim == x.shape[-1]:\n",
        "          shortcut = x\n",
        "        else:\n",
        "          shortcut = ncsn_conv1x1(x, self.output_dim)\n",
        "        h = ncsn_conv3x3(h, self.output_dim)\n",
        "        h = self.normalization()(h, y)\n",
        "        h = self.act(h)\n",
        "        h = ncsn_conv3x3(h, self.output_dim)\n",
        "\n",
        "    return h + shortcut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Gba5FAb0tAOt"
      },
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "MATT:\n",
        "Want to build a U-NET in jax which will have 5 layers (to match Song+2020)\n",
        "Need to take into acount the noise scales - ie embedding the noise scale into the model\n",
        "Build this with flax.linen (flax.linen as nn)?\n",
        "\"\"\"\n",
        "# grabbed from https://github.com/yang-song/score_sde/blob/main/models/ncsnv2.py\n",
        "\n",
        "CondResidualBlock = ConditionalResidualBlock\n",
        "conv3x3 = ncsn_conv3x3\n",
        "\n",
        "class NCSNv2(nn.Module):\n",
        "  \"\"\"NCSNv2 model architecture\"\"\"\n",
        "  #config: ml_collections.ConfigDict\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, labels, train=True):\n",
        "    \n",
        "    # hard coding configs for now\n",
        "    sigma_begin   = 1                     # noise scale max\n",
        "    sigma_end     = 1e-2                  # noise scale min\n",
        "    num_scales    = 10                     # number of noise scales\n",
        "    sigmas = np.exp(np.linspace(np.log(sigma_begin), \n",
        "                      np.log(sigma_end), num_scales))\n",
        "    im_size       = 32                    # image size\n",
        "    nf            = 128                   # number of filters\n",
        "    act           = nn.elu                # activation function\n",
        "    normalizer    = InstanceNorm2dPlus()  # normalization function\n",
        "    interpolation = 'bilinear'            # interpolation method for upsample\n",
        "    \n",
        "    # data already centered\n",
        "    h = x\n",
        "\n",
        "    # --------------------------- #\n",
        "    #    MATT: testing things     #\n",
        "    print(f'size of h: {h.shape}')\n",
        "    # --------------------------- #\n",
        "    \n",
        "    # Begin the U-Net\n",
        "    h = conv3x3(h, nf, stride=1, bias=True)\n",
        "    # ResNet backbone\n",
        "    h = ResidualBlock(nf, resample=None, act=act, normalization=normalizer)(h)\n",
        "    layer1 = ResidualBlock(nf, resample=None, act=act, normalization=normalizer)(h)\n",
        "    h = ResidualBlock(2 * nf, resample='down', act=act, normalization=normalizer)(layer1)\n",
        "    layer2 = ResidualBlock(2 * nf, resample=None, act=act, normalization=normalizer)(h)\n",
        "    h = ResidualBlock(2 * nf,\n",
        "                      resample='down',\n",
        "                      act=act,\n",
        "                      normalization=normalizer,\n",
        "                      dilation=2)(layer2)\n",
        "    layer3 = ResidualBlock(2 * nf, resample=None, act=act, normalization=normalizer, dilation=2)(h)\n",
        "    h = ResidualBlock(2 * nf,\n",
        "                      resample='down',\n",
        "                      act=act,\n",
        "                      normalization=normalizer,\n",
        "                      dilation=4)(layer3)\n",
        "    layer4 = ResidualBlock(2 * nf, resample=None, act=act, normalization=normalizer, dilation=4)(h)\n",
        "    # U-Net with RefineBlocks\n",
        "    ref1 = RefineBlock(layer4.shape[1:3],\n",
        "                       2 * nf,\n",
        "                      act=act,\n",
        "                      interpolation=interpolation,\n",
        "                      start=True)([layer4])\n",
        "    ref2 = RefineBlock(layer3.shape[1:3],\n",
        "                       2 * nf,\n",
        "                      interpolation=interpolation,\n",
        "                      act=act)([layer3, ref1])\n",
        "    ref3 = RefineBlock(layer2.shape[1:3],\n",
        "                       2 * nf,\n",
        "                      interpolation=interpolation,\n",
        "                      act=act)([layer2, ref2])\n",
        "    ref4 = RefineBlock(layer1.shape[1:3],\n",
        "                      nf,\n",
        "                      interpolation=interpolation,\n",
        "                      act=act,\n",
        "                      end=True)([layer1, ref3])\n",
        "\n",
        "    h = normalizer()(ref4)\n",
        "    h = act(h)\n",
        "    h = conv3x3(h, x.shape[-1])\n",
        "\n",
        "    # normlising the output\n",
        "    used_sigmas = sigmas[labels].reshape(\n",
        "        (x.shape[0], *([1] * len(x.shape[1:]))))\n",
        "    return h / used_sigmas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The loss function for a noise dependent score model from Song+2020\n",
        "\"\"\"\n",
        "def anneal_dsm_score_estimation(model, samples, labels, sigmas, key, anneal_power=2.):\n",
        "    sigmas = sigmas[..., None]\n",
        "    noise = jax.random.normal(key, samples.shape)\n",
        "    perturbed_samples = samples + noise * sigmas\n",
        "    target = -noise / sigmas\n",
        "    scores = model(perturbed_samples, labels)\n",
        "    loss = 1 / 2. * ((scores - target) ** 2).sum(axis=-1) * sigmas.squeeze() ** anneal_power\n",
        "    return loss.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MtP7sqAOtAOt",
        "outputId": "d6d3874a-d434-4f51-8e45-cec448828d84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of h: (1, 32, 32, 1)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "__call__() missing 1 required positional argument: 'x'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m model \u001b[39m=\u001b[39m NCSNv2()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#model = model_def()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m variables \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minit({\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m: params_rng}, fake_input, fake_label)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Variables is a `flax.FrozenDict`. It is immutable and respects functional programming\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m init_model_state, initial_params \u001b[39m=\u001b[39m variables\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m)\n",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "\u001b[1;32m/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb Cell 6\u001b[0m in \u001b[0;36mNCSNv2.__call__\u001b[0;34m(self, x, labels, train)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m h \u001b[39m=\u001b[39m conv3x3(h, nf, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# ResNet backbone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m h \u001b[39m=\u001b[39m ResidualBlock(nf, resample\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, act\u001b[39m=\u001b[39;49mact, normalization\u001b[39m=\u001b[39;49mnormalizer)(h)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m layer1 \u001b[39m=\u001b[39m ResidualBlock(nf, resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, act\u001b[39m=\u001b[39mact, normalization\u001b[39m=\u001b[39mnormalizer)(h)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m h \u001b[39m=\u001b[39m ResidualBlock(\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m nf, resample\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdown\u001b[39m\u001b[39m'\u001b[39m, act\u001b[39m=\u001b[39mact, normalization\u001b[39m=\u001b[39mnormalizer)(layer1)\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[1;32m/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb Cell 6\u001b[0m in \u001b[0;36mResidualBlock.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m \u001b[39m@nn\u001b[39m\u001b[39m.\u001b[39mcompact\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=230'>231</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m   h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalization()(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=232'>233</a>\u001b[0m   h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(h)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/score_network_jax/diffusion_jax_HSC.ipynb#W5sZmlsZQ%3D%3D?line=233'>234</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresample \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdown\u001b[39m\u001b[39m'\u001b[39m:\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/homebrew/lib/python3.9/site-packages/flax/linen/module.py:732\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m    731\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m--> 732\u001b[0m     y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    733\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    734\u001b[0m   y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'x'"
          ]
        }
      ],
      "source": [
        "\"\"\" \n",
        "The training of the NCSNv2 model. Here define the training\n",
        "parameters and initialise the model. Train on a small scale \n",
        "for testing before moving to the full scale on GPU HPC.\n",
        "\"\"\"\n",
        "# ----------- #\n",
        "# model setup #\n",
        "# ----------- #\n",
        "\n",
        "# load in data  \n",
        "box_size = 31\n",
        "dataname = 'sources_box' + str(box_size) + '.npy'     \n",
        "dataset = np.load(dataname)\n",
        "#plt.imshow(dataset[2], cmap='gray')\n",
        "#plt.show()\n",
        "\n",
        "# perform zero-padding of the data to get desired dimensions\n",
        "data_padded_31 = []\n",
        "#dataset = np.resize(dataset,(1989,96,96))\n",
        "for i in range(len(dataset)):\n",
        "    data_padded_tmp = np.pad(dataset[i], ((0,1),(0,1)), 'constant')\n",
        "    data_padded_31.append(data_padded_tmp)\n",
        "dataset = np.array( data_padded_31 )\n",
        "\n",
        "# define noise levels \n",
        "sigma_begin = 1\n",
        "sigma_end   = 0.01\n",
        "num_scales  = 10\n",
        "sigmas      = np.exp(np.linspace(np.log(sigma_begin), \n",
        "                        np.log(sigma_end), num_scales))\n",
        "\n",
        "# score model params\n",
        "n_epochs    = 50                                    # number of epochs\n",
        "steps       = 1_000                                 # number of steps per epoch\n",
        "batch_size  = 32                                    # batch size\n",
        "lr          = 1e-4                                  # learning rate\n",
        "rng         = jax.random.PRNGKey(1992)              # random seed\n",
        "input_shape = (jax.local_device_count(), 32, 32, 1) # size 32 by 32 one channel\n",
        "label_shape = input_shape[:1]\n",
        "fake_input  = jnp.zeros(input_shape)\n",
        "fake_label  = jnp.zeros(label_shape, dtype=jnp.int32)\n",
        "params_rng, dropout_rng = jax.random.split(rng)\n",
        "model = NCSNv2()\n",
        "#model = model_def()\n",
        "variables = model.init({'params': params_rng}, fake_input, fake_label)\n",
        "# Variables is a `flax.FrozenDict`. It is immutable and respects functional programming\n",
        "init_model_state, initial_params = variables.pop('params')\n",
        "optimizer = flax.optim.Adam(learning_rate=lr,\n",
        "                            beta1 = 0.9,\n",
        "                            eps = 1e-8).create(initial_params)  # create optimizer\n",
        "\n",
        "# ------------- #\n",
        "# training loop #\n",
        "# ------------- #\n",
        "@jax.jit\n",
        "def train_step(model, optimizer, rng, samples, labels, sigmas):\n",
        "    rng   = jax.random.PRNGKey(rng) # random number random seed\n",
        "    grads = jax.grad(anneal_dsm_score_estimation)(model, samples, labels, sigmas, rng)\n",
        "    model = optimizer.update(grads, model)\n",
        "    return model, optimizer, rng\n",
        "\n",
        "key_seq = jax.random.PRNGKey(0)\n",
        "for t in tqdm(range(steps + 1)):\n",
        "\n",
        "    idx = np.random.randint(0, len(dataset))\n",
        "    labels = np.random.randint(0, len(sigmas), size=len(dataset[idx])) # size for 32 by 32\n",
        "    model, optimizer, key_seq = train_step(model, optimizer, \n",
        "                                key_seq, dataset[idx], labels, sigmas[labels])\n",
        "\n",
        "    if ((t % (steps // 5)) == 0):\n",
        "        labels = np.random.randint(0, len(sigmas), size=len(dataset[0]))\n",
        "        print(anneal_dsm_score_estimation(model, dataset[0], labels, sigmas[labels], rng))\n",
        "# -------------------- #\n",
        "# end of training loop #       \n",
        "# -------------------- #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "CallCompactUnboundModuleError",
          "evalue": "Can't call compact methods on unbound modules (https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.CallCompactUnboundModuleError)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCallCompactUnboundModuleError\u001b[0m             Traceback (most recent call last)",
            "\u001b[1;32m/Users/mattsampson/Princeton/research/HSC_galaxies/diffusion_modelling/diffusion_jax_HSC.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/HSC_galaxies/diffusion_modelling/diffusion_jax_HSC.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m galaxy \u001b[39m=\u001b[39m dataset[\u001b[39m1992\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/HSC_galaxies/diffusion_modelling/diffusion_jax_HSC.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sigmas), (gaussian_noise\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/HSC_galaxies/diffusion_modelling/diffusion_jax_HSC.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m scores \u001b[39m=\u001b[39m model(gaussian_noise, labels)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/HSC_galaxies/diffusion_modelling/diffusion_jax_HSC.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m scores2 \u001b[39m=\u001b[39m model(galaxy, labels)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mattsampson/Princeton/research/HSC_galaxies/diffusion_modelling/diffusion_jax_HSC.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m fig , ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,figsize\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m, \u001b[39m5.5\u001b[39m), facecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mblack\u001b[39m\u001b[39m'\u001b[39m,dpi \u001b[39m=\u001b[39m \u001b[39m70\u001b[39m)\n",
            "File \u001b[0;32m~/homebrew/lib/python3.9/site-packages/flax/linen/transforms.py:1228\u001b[0m, in \u001b[0;36mnamed_call.<locals>.wrapped_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m module_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m   1227\u001b[0m full_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mmethod_suffix\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1228\u001b[0m \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39;49mnamed_call(class_fn, name\u001b[39m=\u001b[39;49mfull_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
            "File \u001b[0;32m~/homebrew/lib/python3.9/site-packages/flax/linen/module.py:350\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    349\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 350\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    351\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/homebrew/lib/python3.9/site-packages/flax/linen/module.py:643\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39mif\u001b[39;00m is_compact_method:\n\u001b[1;32m    642\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 643\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mCallCompactUnboundModuleError()\n\u001b[1;32m    644\u001b[0m   is_recurrent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39min_compact_method\n\u001b[1;32m    645\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39min_compact_method \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[0;31mCallCompactUnboundModuleError\u001b[0m: Can't call compact methods on unbound modules (https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.CallCompactUnboundModuleError)"
          ]
        }
      ],
      "source": [
        "# ------------------------ #\n",
        "# testing score estimation #\n",
        "# ------------------------ #\n",
        "gaussian_noise = jax.random.normal(rng, shape=(32,32))\n",
        "galaxy = dataset[1992]\n",
        "labels = np.random.randint(0, len(sigmas), (gaussian_noise.shape[0],))\n",
        "scores = model(gaussian_noise, labels)\n",
        "scores2 = model(galaxy, labels)\n",
        "fig , ax = plt.subplots(1,2,figsize=(16, 5.5), facecolor='black',dpi = 70)\n",
        "plt.subplots_adjust(wspace=0.01)\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(scores, cmap='plasma')\n",
        "#plt.colorbar()\n",
        "plt.title('Gaussian Noise',fontsize=28,pad=15)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(scores2, cmap='plasma')\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label(r'$\\nabla_x log \\ p(\\mathbf{\\tilde{x}})$', rotation=270, fontsize = 20,labelpad= 25)\n",
        "plt.title('Galaxy',fontsize=28,pad=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXfpd5tptAOt",
        "outputId": "72601a87-db35-4a44-91d6-64744b9fc1fb"
      },
      "outputs": [],
      "source": [
        "# ------------------------- #\n",
        "# langevin dynamic sampling #\n",
        "# ------------------------- #\n",
        "# TODO: port to jax\n",
        "\n",
        "def anneal_Langevin_dynamics(x_mod, scorenet, sigmas, n_steps_each=100, step_lr=0.000008,\n",
        "                             final_only=False, verbose=False, denoise=True):\n",
        "    images = []\n",
        "    scores  = []\n",
        "\n",
        "    for c, sigma in enumerate(sigmas):\n",
        "        labels = torch.ones(x_mod.shape[0], device=x_mod.device) * c\n",
        "        labels = labels.long()\n",
        "        step_size = step_lr * (sigma / sigmas[-1]) ** 2\n",
        "        step_size_cpu = step_size.to('cpu') \n",
        "        for s in range(n_steps_each):\n",
        "            grad = scorenet(x_mod, labels)\n",
        "            scores.append(grad.to('cpu'))\n",
        "            noise = torch.randn_like(x_mod)\n",
        "            grad_norm = torch.norm(grad.view(grad.shape[0], -1), dim=-1).mean()\n",
        "            noise_norm = torch.norm(noise.view(noise.shape[0], -1), dim=-1).mean()\n",
        "            x_mod = x_mod + step_size_cpu * grad + noise * np.sqrt(step_size_cpu * 2)\n",
        "\n",
        "            if not final_only:\n",
        "                images.append(x_mod.to('cpu'))\n",
        "\n",
        "    if denoise:\n",
        "        last_noise = (len(sigmas) - 1) * torch.ones(x_mod.shape[0], device=x_mod.device)\n",
        "        last_noise = last_noise.long()\n",
        "        x_mod = x_mod + sigmas[-1] ** 2 * scorenet(x_mod, last_noise)\n",
        "        images.append(x_mod.to('cpu'))\n",
        "\n",
        "    return images, scores"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "vscode": {
      "interpreter": {
        "hash": "3bc68688a3ed8a5ce6018ce4ac9bdddfcbacccfa8757535ca0e210f8a40fad74"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
