{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device found is: cpu\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------------------ #\n",
        "# A jax implementation of the diffusion model from                               #\n",
        "# the paper \"Improved Techniques for Training Score-Based Generative Models\"     #\n",
        "# https://arxiv.org/abs/2006.09011                                               #\n",
        "# Code taken primarily from https://github.com/yang-song/score_sde/              #\n",
        "# Author: Matt Sampson                                                           #\n",
        "# Created: 2023                                                                  #\n",
        "#                                                                                #\n",
        "# Main modifications by Matt Sampson include:                                    #\n",
        "#    - Minor updates to U-NET use the latest version of JAX/flax.linen           #\n",
        "#    - removal of config files all params define in python file                  #\n",
        "#    - data loader added using numpy arrays then converted to jax arrays         #\n",
        "#    - changed optim.Adam to optax.adam (required for latex flax)                # \n",
        "#    - re-wrote optimisation routine to use optax                                #\n",
        "#    - replaced the training loop with mini-batch grad descent in optax          #\n",
        "#    - re-wrote Langevin sampler in JAX soon with JIT (much faster sampling)     #\n",
        "#    - addition of various visualisation routines                                #\n",
        "# ------------------------------------------------------------------------------ #\n",
        "\n",
        "#%matplotlib inline\n",
        "import functools\n",
        "import math\n",
        "import string\n",
        "from typing import Any, Sequence, Optional\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.nn as jnn\n",
        "import jax.numpy as jnp\n",
        "import jax.nn.initializers as init\n",
        "import matplotlib as mpl\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import flax\n",
        "import optax\n",
        "from jax import jit\n",
        "from tqdm import tqdm\n",
        "Path(\"outputs\").mkdir(exist_ok=True)\n",
        "\n",
        "# test we can find correct device\n",
        "from jax.lib import xla_bridge\n",
        "print(f'Device found is: {xla_bridge.get_backend().platform}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Common layers for defining score networks.\n",
        "\"\"\"\n",
        "class InstanceNorm2dPlus(nn.Module):\n",
        "  \"\"\"InstanceNorm++ as proposed in the original NCSN paper.\"\"\"\n",
        "  bias: bool = True\n",
        "\n",
        "  @staticmethod\n",
        "  def scale_init(key, shape, dtype=jnp.float32):\n",
        "    normal_init = init.normal(0.02)\n",
        "    return normal_init(key, shape, dtype=dtype) + 1.\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    means = jnp.mean(x, axis=(1, 2))\n",
        "    m = jnp.mean(means, axis=-1, keepdims=True)\n",
        "    v = jnp.var(means, axis=-1, keepdims=True)\n",
        "    means_plus = (means - m) / jnp.sqrt(v + 1e-5)\n",
        "\n",
        "    h = (x - means[:, None, None, :]) / jnp.sqrt(jnp.var(x, axis=(1, 2), keepdims=True) + 1e-5)\n",
        "\n",
        "    h = h + means_plus[:, None, None, :] * self.param('alpha', InstanceNorm2dPlus.scale_init, (1, 1, 1, x.shape[-1]))\n",
        "    h = h * self.param('gamma', InstanceNorm2dPlus.scale_init, (1, 1, 1, x.shape[-1]))\n",
        "    if self.bias:\n",
        "      h = h + self.param('beta', init.zeros, (1, 1, 1, x.shape[-1]))\n",
        "\n",
        "    return h\n",
        "\n",
        "\n",
        "def ncsn_conv1x1(x, out_planes, stride=1, bias=True, dilation=1, init_scale=1.):\n",
        "  \"\"\"1x1 convolution with PyTorch initialization. Same as NCSNv1/v2.\"\"\"\n",
        "  init_scale = 1e-10 if init_scale == 0 else init_scale\n",
        "  kernel_init = jnn.initializers.variance_scaling(1 / 3 * init_scale, 'fan_in',\n",
        "                                                  'uniform')\n",
        "  kernel_shape = (1, 1) + (x.shape[-1], out_planes)\n",
        "  bias_init = lambda key, shape: kernel_init(key, kernel_shape)[0, 0, 0, :]\n",
        "  output = nn.Conv(out_planes, kernel_size=(1, 1),\n",
        "                   strides=(stride, stride), padding='SAME', use_bias=bias,\n",
        "                   kernel_dilation=(dilation, dilation),\n",
        "                   kernel_init=kernel_init,\n",
        "                   bias_init=bias_init)(x)\n",
        "  return output\n",
        "\n",
        "def ncsn_conv3x3(x, out_planes, stride=1, bias=True, dilation=1, init_scale=1.):\n",
        "  \"\"\"3x3 convolution with PyTorch initialization. Same as NCSNv1/NCSNv2.\"\"\"\n",
        "  kernel_init = jnn.initializers.variance_scaling(1 / 3 * init_scale, 'fan_in',\n",
        "                                                  'uniform')\n",
        "  kernel_shape = (3, 3) + (x.shape[-1], out_planes)\n",
        "  #bias_init = lambda key, shape: kernel_init(key, kernel_shape)[0, 0, 0, :]\n",
        "  bias_init = jnn.initializers.zeros\n",
        "  output = nn.Conv(out_planes,\n",
        "                   kernel_size=(3, 3),\n",
        "                   strides=(stride, stride),\n",
        "                   padding='SAME',\n",
        "                   use_bias= bias,\n",
        "                   kernel_dilation=(dilation, dilation),\n",
        "                   kernel_init=kernel_init,\n",
        "                   bias_init=bias_init)(x)\n",
        "  return output\n",
        "\n",
        "# --------------------------------------------------------- #\n",
        "# Functions below are ported over from the NCSNv2 codebase: #\n",
        "#        https://github.com/ermongroup/ncsnv2               #\n",
        "# --------------------------------------------------------- #\n",
        "\n",
        "class CRPBlock(nn.Module):\n",
        "  \"\"\"CRPBlock for RefineNet. Used in NCSNv2.\"\"\"\n",
        "  features: int\n",
        "  n_stages: int\n",
        "  act: Any = nn.relu\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = self.act(x)\n",
        "    path = x\n",
        "    for _ in range(self.n_stages):\n",
        "      path = nn.max_pool(\n",
        "        path, window_shape=(5, 5), strides=(1, 1), padding='SAME')\n",
        "      path = ncsn_conv3x3(path, self.features, stride=1, bias=False)\n",
        "      x = path + x\n",
        "    return x\n",
        "\n",
        "\n",
        "class RCUBlock(nn.Module):\n",
        "  \"\"\"RCUBlock for RefineNet. Used in NCSNv2.\"\"\"\n",
        "  features: int\n",
        "  n_blocks: int\n",
        "  n_stages: int\n",
        "  act: Any = nn.relu\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    for _ in range(self.n_blocks):\n",
        "      residual = x\n",
        "      for _ in range(self.n_stages):\n",
        "        x = self.act(x)\n",
        "        x = ncsn_conv3x3(x, self.features, stride=1, bias=False)\n",
        "      x = x + residual\n",
        "\n",
        "    return x\n",
        "\n",
        "class MSFBlock(nn.Module):\n",
        "  \"\"\"MSFBlock for RefineNet. Used in NCSNv2.\"\"\"\n",
        "  shape: Sequence[int]\n",
        "  features: int\n",
        "  interpolation: str = 'bilinear'\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, xs):\n",
        "    sums = jnp.zeros((xs[0].shape[0], *self.shape, self.features))\n",
        "    for i in range(len(xs)):\n",
        "      h = ncsn_conv3x3(xs[i], self.features, stride=1, bias=True)\n",
        "      if self.interpolation == 'bilinear':\n",
        "        h = jax.image.resize(h, (h.shape[0], *self.shape, h.shape[-1]), 'bilinear')\n",
        "      elif self.interpolation == 'nearest_neighbor':\n",
        "        h = jax.image.resize(h, (h.shape[0], *self.shape, h.shape[-1]), 'nearest')\n",
        "      else:\n",
        "        raise ValueError(f'Interpolation {self.interpolation} does not exist!')\n",
        "      sums = sums + h\n",
        "    return sums\n",
        "\n",
        "class RefineBlock(nn.Module):\n",
        "  \"\"\"RefineBlock for building NCSNv2 RefineNet.\"\"\"\n",
        "  output_shape: Sequence[int]\n",
        "  features: int\n",
        "  act: Any = nn.relu\n",
        "  interpolation: str = 'bilinear'\n",
        "  start: bool = False\n",
        "  end: bool = False\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, xs):\n",
        "    rcu_block = functools.partial(RCUBlock, n_blocks=2, n_stages=2, act=self.act)\n",
        "    rcu_block_output = functools.partial(RCUBlock,\n",
        "                                        features=self.features,\n",
        "                                        n_blocks=3 if self.end else 1,\n",
        "                                        n_stages=2,\n",
        "                                        act=self.act)\n",
        "    hs = []\n",
        "    for i in range(len(xs)):\n",
        "      h = rcu_block(features=xs[i].shape[-1])(xs[i])\n",
        "      hs.append(h)\n",
        "\n",
        "    if not self.start:\n",
        "      msf = functools.partial(MSFBlock, features=self.features, interpolation=self.interpolation)\n",
        "      h = msf(shape=self.output_shape)(hs)\n",
        "    else:\n",
        "      h = hs[0]\n",
        "\n",
        "    crp = functools.partial(CRPBlock, features=self.features, n_stages=2, act=self.act)\n",
        "    h = crp()(h)\n",
        "    h = rcu_block_output()(h)\n",
        "    return h\n",
        "\n",
        "class ConvMeanPool(nn.Module):\n",
        "  \"\"\"ConvMeanPool for building the ResNet backbone.\"\"\"\n",
        "  output_dim: int\n",
        "  kernel_size: int = 3\n",
        "  biases: bool = True\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    output = nn.Conv(features=self.output_dim,\n",
        "                    kernel_size=(self.kernel_size, self.kernel_size),\n",
        "                    strides=(1, 1),\n",
        "                    padding='SAME',\n",
        "                    use_bias=self.biases)(inputs)\n",
        "    output = sum([\n",
        "      output[:, ::2, ::2, :], output[:, 1::2, ::2, :],\n",
        "      output[:, ::2, 1::2, :], output[:, 1::2, 1::2, :]\n",
        "    ]) / 4.\n",
        "    return output\n",
        "\n",
        "\n",
        "class MeanPoolConv(nn.Module):\n",
        "  \"\"\"MeanPoolConv for building the ResNet backbone.\"\"\"\n",
        "  output_dim: int\n",
        "  kernel_size: int = 3\n",
        "  biases: bool = True\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    output = inputs\n",
        "    output = sum([\n",
        "      output[:, ::2, ::2, :], output[:, 1::2, ::2, :],\n",
        "      output[:, ::2, 1::2, :], output[:, 1::2, 1::2, :]\n",
        "    ]) / 4.\n",
        "    output = nn.Conv(\n",
        "      features=self.output_dim,\n",
        "      kernel_size=(self.kernel_size, self.kernel_size),\n",
        "      strides=(1, 1),\n",
        "      padding='SAME',\n",
        "      use_bias=self.biases)(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  \"\"\"The residual block for defining the ResNet backbone. Used in NCSNv2.\"\"\"\n",
        "  output_dim: int\n",
        "  normalization: Any\n",
        "  resample: Optional[str] = None\n",
        "  act: Any = nn.elu\n",
        "  dilation: int = 1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    h = self.normalization()(x)\n",
        "    h = self.act(h)\n",
        "    if self.resample == 'down':\n",
        "      h = ncsn_conv3x3(h, h.shape[-1], dilation=self.dilation)\n",
        "      h = self.normalization()(h)\n",
        "      h = self.act(h)\n",
        "      if self.dilation > 1:\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "        shortcut = ncsn_conv3x3(x, self.output_dim, dilation=self.dilation)\n",
        "      else:\n",
        "        h = ConvMeanPool(output_dim=self.output_dim)(h)\n",
        "        shortcut = ConvMeanPool(output_dim=self.output_dim, kernel_size=1)(x)\n",
        "    elif self.resample is None:\n",
        "      if self.dilation > 1:\n",
        "        if self.output_dim == x.shape[-1]:\n",
        "          shortcut = x\n",
        "        else:\n",
        "          shortcut = ncsn_conv3x3(x, self.output_dim, dilation=self.dilation)\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "        h = self.normalization()(h)\n",
        "        h = self.act(h)\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "      else:\n",
        "        if self.output_dim == x.shape[-1]:\n",
        "          shortcut = x\n",
        "        else:\n",
        "          shortcut = ncsn_conv1x1(x, self.output_dim)\n",
        "        h = ncsn_conv3x3(h, self.output_dim)\n",
        "        h = self.normalization()(h)\n",
        "        h = self.act(h)\n",
        "        h = ncsn_conv3x3(h, self.output_dim)\n",
        "\n",
        "    return h + shortcut\n",
        "\n",
        "class ConditionalResidualBlock(nn.Module):\n",
        "  \"\"\"The noise-conditional residual block for building NCSNv1.\"\"\"\n",
        "  output_dim: int\n",
        "  normalization: Any\n",
        "  resample: Optional[str] = None\n",
        "  act: Any = nn.elu\n",
        "  dilation: int = 1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, y):\n",
        "    h = self.normalization()(x, y)\n",
        "    h = self.act(h)\n",
        "    if self.resample == 'down':\n",
        "      h = ncsn_conv3x3(h, h.shape[-1], dilation=self.dilation)\n",
        "      h = self.normalization(h, y)\n",
        "      h = self.act(h)\n",
        "      if self.dilation > 1:\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "        shortcut = ncsn_conv3x3(x, self.output_dim, dilation=self.dilation)\n",
        "      else:\n",
        "        h = ConvMeanPool(output_dim=self.output_dim)(h)\n",
        "        shortcut = ConvMeanPool(output_dim=self.output_dim, kernel_size=1)(x)\n",
        "    elif self.resample is None:\n",
        "      if self.dilation > 1:\n",
        "        if self.output_dim == x.shape[-1]:\n",
        "          shortcut = x\n",
        "        else:\n",
        "          shortcut = ncsn_conv3x3(x, self.output_dim, dilation=self.dilation)\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "        h = self.normalization()(h, y)\n",
        "        h = self.act(h)\n",
        "        h = ncsn_conv3x3(h, self.output_dim, dilation=self.dilation)\n",
        "      else:\n",
        "        if self.output_dim == x.shape[-1]:\n",
        "          shortcut = x\n",
        "        else:\n",
        "          shortcut = ncsn_conv1x1(x, self.output_dim)\n",
        "        h = ncsn_conv3x3(h, self.output_dim)\n",
        "        h = self.normalization()(h, y)\n",
        "        h = self.act(h)\n",
        "        h = ncsn_conv3x3(h, self.output_dim)\n",
        "\n",
        "    return h + shortcut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gba5FAb0tAOt"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------------------------- #\n",
        "# JAX port of ncsnv2 - updated to work with                                   #\n",
        "# latest JAX/FLAX versions using optax over flax.optim                        #\n",
        "# original: https://github.com/yang-song/score_sde/blob/main/models/ncsnv2.py #\n",
        "# --------------------------------------------------------------------------- #\n",
        "CondResidualBlock = ConditionalResidualBlock\n",
        "conv3x3 = ncsn_conv3x3\n",
        "\n",
        "class NCSNv2(nn.Module):\n",
        "  \"\"\"NCSNv2 model architecture\"\"\"\n",
        "  #config: ml_collections.ConfigDict\n",
        "\n",
        "  @nn.compact\n",
        "  #def __call__(self, x, labels, train=True):\n",
        "  def __call__(self, x, labels):\n",
        "    \n",
        "    # hard coding configs for now\n",
        "    sigma_begin   = 1                     # noise scale max\n",
        "    sigma_end     = 1e-2                  # noise scale min\n",
        "    num_scales    = 10                     # number of noise scales\n",
        "    sigmas        = jnp.exp(jnp.linspace(jnp.log(sigma_end), \n",
        "                              jnp.log(sigma_begin),num_scales))\n",
        "    sigmas = jax.numpy.flip(sigmas)\n",
        "    im_size       = 32                    # image size\n",
        "    nf            = 128                   # number of filters\n",
        "    act           = nn.elu                # activation function\n",
        "    normalizer    = InstanceNorm2dPlus    # normalization function\n",
        "    interpolation = 'bilinear'            # interpolation method for upsample\n",
        "    data_centered = False                # whether data is already centered\n",
        "    \n",
        "    # data already centered\n",
        "    if not data_centered:\n",
        "      h = 2 * x - 1.\n",
        "    else:\n",
        "      h = x\n",
        "    \n",
        "    # Begin the U-Net\n",
        "    h = conv3x3(h, nf, stride=1, bias=True)\n",
        "    \n",
        "    # TODO: check the size of nf is correct\n",
        "\n",
        "    # ResNet backbone\n",
        "    h = ResidualBlock(nf, resample=None, act=act, normalization=normalizer)(h)\n",
        "    layer1 = ResidualBlock(nf, resample=None, act=act, normalization=normalizer)(h)\n",
        "    h = ResidualBlock(2 * nf, resample='down', act=act, normalization=normalizer)(layer1)\n",
        "    layer2 = ResidualBlock(2 * nf, resample=None, act=act, normalization=normalizer)(h)\n",
        "    h = ResidualBlock(2 * nf,\n",
        "                      resample='down',\n",
        "                      act=act,\n",
        "                      normalization=normalizer,\n",
        "                      dilation=2)(layer2)\n",
        "    layer3 = ResidualBlock(2 * nf, resample=None, act=act, normalization=normalizer, dilation=2)(h)\n",
        "    h = ResidualBlock(2 * nf,\n",
        "                      resample='down',\n",
        "                      act=act,\n",
        "                      normalization=normalizer,\n",
        "                      dilation=4)(layer3)\n",
        "    layer4 = ResidualBlock(2 * nf, resample=None, act=act, normalization=normalizer, dilation=4)(h)\n",
        "    # U-Net with RefineBlocks\n",
        "    ref1 = RefineBlock(layer4.shape[1:3],\n",
        "                       2 * nf,\n",
        "                      act=act,\n",
        "                      interpolation=interpolation,\n",
        "                      start=True)([layer4])\n",
        "    ref2 = RefineBlock(layer3.shape[1:3],\n",
        "                       2 * nf,\n",
        "                      interpolation=interpolation,\n",
        "                      act=act)([layer3, ref1])\n",
        "    ref3 = RefineBlock(layer2.shape[1:3],\n",
        "                       2 * nf,\n",
        "                      interpolation=interpolation,\n",
        "                      act=act)([layer2, ref2])\n",
        "    ref4 = RefineBlock(layer1.shape[1:3],\n",
        "                      nf,\n",
        "                      interpolation=interpolation,\n",
        "                      act=act,\n",
        "                      end=True)([layer1, ref3])\n",
        "\n",
        "    h = normalizer()(ref4)\n",
        "    h = act(h)\n",
        "    h = conv3x3(h, x.shape[-1])\n",
        "\n",
        "    # normlising the output\n",
        "    used_sigmas = sigmas[labels].reshape(\n",
        "        (x.shape[0], *([1] * len(x.shape[1:]))))\n",
        "    \n",
        "    return h / used_sigmas\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------- #\n",
        "# The loss function for a noise dependent score model        #\n",
        "#  defined Song+2021 https://arxiv.org/abs/2006.09011 eqn: 2 #           \n",
        "# updated for latest JAX                                     #\n",
        "# ---------------------------------------------------------- #\n",
        "def anneal_dsm_score_estimation(params, model, samples, labels, sigmas, key):\n",
        "    \"\"\"\n",
        "    Loss function for annealed score estimation\n",
        "    -------------------------------------------\n",
        "    Inputs: params:  - the model parameters\n",
        "            model:   - the score neural network\n",
        "            samples: - the samples from the data distribution\n",
        "            labels:  - the noise scale labels\n",
        "            sigmas:  - the noise scales\n",
        "            key:     - the jax random key\n",
        "\n",
        "    Output: loss - the loss value\n",
        "    -------------------------------------------\n",
        "    \"\"\"\n",
        "    used_sigmas = sigmas[labels].reshape((samples.shape[0], \n",
        "                                          *([1] * len(samples.shape[1:]))))\n",
        "    noise = jax.random.normal(key, samples.shape)\n",
        "    perturbed_samples = samples + noise * used_sigmas\n",
        "    target = -noise / used_sigmas**2\n",
        "    scores = model.apply({'params': params}, perturbed_samples, labels)\n",
        "    #losses = jnp.square(score - target)\n",
        "    loss_1 = 1 / 2. * ((scores - target) ** 2).sum(axis=-1) #* used_sigmas.squeeze() ** anneal_power\n",
        "    loss = loss_1 * used_sigmas**2 \n",
        "    loss = jnp.mean(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MtP7sqAOtAOt",
        "outputId": "d6d3874a-d434-4f51-8e45-cec448828d84"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------ #\n",
        "# The training of the NCSNv2 model. Here define the training   #\n",
        "# parameters and initialise the model. Train on a small scale  #\n",
        "# for testing before moving to the full scale on GPU HPC.      #\n",
        "# ------------------------------------------------------------ #\n",
        "\n",
        "# load in data  low res\n",
        "\n",
        "box_size = 31\n",
        "dataname = 'sources_box' + str(box_size) + '.npy'     \n",
        "dataset = np.load(dataname)\n",
        "\n",
        "# perform zero-padding of the data to get desired dimensions\n",
        "data_padded_31 = []\n",
        "for i in range(len(dataset)):\n",
        "    data_padded_tmp = np.pad(dataset[i], ((0,1),(0,1)), 'constant')\n",
        "    data_padded_31.append(data_padded_tmp)\n",
        "dataset = np.array( data_padded_31 )\n",
        "\n",
        "\"\"\"\n",
        "# load in data  high res\n",
        "box_size = 61\n",
        "dataname = 'sources_box' + str(box_size) + '.npy'     \n",
        "dataset = np.load(dataname)\n",
        "\n",
        "# perform zero-padding of the data to get desired dimensions\n",
        "data_padded_61 = []\n",
        "for i in range(len(dataset)):\n",
        "    data_padded_tmp = np.pad(dataset[i], ((1,2),(1,2)), 'constant')\n",
        "    data_padded_61.append(data_padded_tmp)\n",
        "dataset = np.array( data_padded_61 )\n",
        "\"\"\"\n",
        "# convert dataset to jax array\n",
        "dataset = np.expand_dims(dataset, axis=-1)\n",
        "data_jax = jnp.array(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------ #\n",
        "# visualisation for code testing #\n",
        "# ------------------------------ #\n",
        "\n",
        "import cmasher as cmr\n",
        "score_map = cmr.iceburn\n",
        "data_map = cmr.ember\n",
        "def plot_evolve(params,sample,step, labels):\n",
        "    gaussian_noise = jax.random.normal(key_seq, shape=sample.shape)\n",
        "    scores = model.apply({'params' : params}, gaussian_noise, labels)\n",
        "    scores2 = model.apply({'params' : params}, sample, labels)\n",
        "    fig , ax = plt.subplots(2,2,figsize=(16, 12), facecolor='white',dpi = 70)\n",
        "    plt.subplots_adjust(wspace=0.01)\n",
        "    plt.subplot(2,2,1)\n",
        "    plt.imshow(scores[0], cmap=score_map)\n",
        "    #plt.colorbar()\n",
        "    plt.title('Gaussian Noise',fontsize=36,pad=15)\n",
        "    plt.ylabel('score', fontsize=40)\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.imshow(scores2[0], cmap=score_map)\n",
        "    cbar = plt.colorbar()\n",
        "    cbar.set_label(r'$\\nabla_x log \\ p(\\mathbf{\\tilde{x}})$', rotation=270, fontsize = 20,labelpad= 25)\n",
        "    plt.title('Galaxy',fontsize=36,pad=15)\n",
        "    plt.subplot(2,2,3)\n",
        "    plt.imshow(gaussian_noise[0], cmap=data_map)\n",
        "    plt.ylabel('data', fontsize=40)\n",
        "    plt.subplot(2,2,4)\n",
        "    plt.imshow(sample[0], cmap=data_map)\n",
        "    cbar = plt.colorbar()\n",
        "    cbar.set_label(r'pixel density', rotation=270, fontsize = 20,labelpad= 25)\n",
        "    super_name = 'training step ' + str(step) \n",
        "    plt.suptitle(super_name, fontsize = 40)\n",
        "    plt.tight_layout()\n",
        "    save_name = 'score_estimation_pre_training_step_' + str(step) + '.png'\n",
        "    plt.savefig(save_name,facecolor='white',dpi=200)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training model:  40%|████      | 2/5 [01:43<02:34, 51.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss at step 1: 2561621.25 loss at prev step 2644516.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training model:  60%|██████    | 3/5 [02:34<01:42, 51.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss at step 2: 2468753.5 loss at prev step 2561621.25\n"
          ]
        }
      ],
      "source": [
        "# ------------------- #\n",
        "# model training step #\n",
        "# ------------------- #\n",
        "\n",
        "# model training and init params\n",
        "key_seq     = jax.random.PRNGKey(42)                # random seed\n",
        "n_epochs    = 3                                    # number of epochs\n",
        "batch_size  = 32                                    # batch size\n",
        "lr          = 1e-4                                  # learning rate\n",
        "im_size     = 32                                    # image size\n",
        "\n",
        "# construct the training data \n",
        "# for testing limit size until GPU HPC is available\n",
        "#data_jax = data_jax[0:200] # DELETE for full training\n",
        "batch = jnp.array(range(0, batch_size))\n",
        "training_data_init = data_jax[batch]\n",
        "batch_per_epoch = len(data_jax) // batch_size\n",
        "\n",
        "# define noise levels and noise params\n",
        "sigma_begin = 1\n",
        "sigma_end   = 0.01\n",
        "num_scales  = 10\n",
        "sigmas      = jnp.exp(jnp.linspace(jnp.log(sigma_end), \n",
        "                        jnp.log(sigma_begin),num_scales))\n",
        "sigmas = jax.numpy.flip(sigmas)\n",
        "labels = jax.random.randint(key_seq, (len(training_data_init),), \n",
        "                            minval=0, maxval=len(sigmas), dtype=jnp.int32)\n",
        "\n",
        "# model init variables\n",
        "input_shape = training_data_init.shape\n",
        "label_shape = labels.shape\n",
        "fake_input  = jnp.zeros(input_shape)\n",
        "fake_label  = jnp.zeros(label_shape, dtype=jnp.int32)\n",
        "params_rng, dropout_rng = jax.random.split(key_seq)\n",
        "\n",
        "# define and initialise model\n",
        "model = NCSNv2()\n",
        "variables = model.init({'params': params_rng}, fake_input, fake_label)\n",
        "# TODO: do I want dropout? If so also edit optimiser add dropout rng to optax args\n",
        "# variables = model.init({'params': params_rng, 'dropout': dropout_rng}, fake_input, fake_label)\n",
        "init_model_state, initial_params = variables.pop('params')\n",
        "\n",
        "# define optimiser using latest flax standards\n",
        "optimizer = optax.adam( learning_rate=lr, \n",
        "                        b1=0.9, \n",
        "                        b2=0.999, \n",
        "                        eps=1e-08, \n",
        "                        eps_root=0.0, \n",
        "                        mu_dtype=None ) \n",
        "\n",
        "# initialise model state\n",
        "params = initial_params\n",
        "model_state = optimizer.init(params)\n",
        "\n",
        "# name loss function\n",
        "loss_fn = anneal_dsm_score_estimation\n",
        "\n",
        "# training settings\n",
        "train       = True\n",
        "plot_scores = False\n",
        "plot_loss   = True\n",
        "verbose     = True\n",
        "best_loss   = 1e15\n",
        "epoch_loss  = 0\n",
        "\n",
        "# TODO: make updates to store and save the model state opposed to the model params\n",
        "# begin training loop storing params\n",
        "if train:\n",
        "  loss_vector = np.zeros(n_epochs)\n",
        "  for i in tqdm(range(n_epochs), desc='training model'):\n",
        "    for batch_idx in range(batch_per_epoch):\n",
        "      \n",
        "      # set up batch and noise samples\n",
        "      batch_length = jnp.array(range(batch_idx*batch_size, (batch_idx+1)*batch_size))\n",
        "      samples = data_jax[batch_length]\n",
        "      labels = jax.random.randint(key_seq, (len(samples),), \n",
        "                            minval=0, maxval=len(sigmas), dtype=jnp.int32)\n",
        "      \n",
        "      # calculate gradients and loss\n",
        "      loss, grads = jax.value_and_grad(loss_fn)(params, model, samples, labels, sigmas, key_seq)\n",
        "      epoch_loss += loss\n",
        "      \n",
        "      # update the model params\n",
        "      updates, model_state = optimizer.update(grads, model_state)\n",
        "      params = optax.apply_updates(params, updates)\n",
        "      \n",
        "    # store epoch loss and make plots\n",
        "    loss_vector[i] = epoch_loss\n",
        "    if loss_vector[i] < best_loss:\n",
        "      best_params = params\n",
        "      best_loss   = loss_vector[i]\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # plots and printing outputs\n",
        "    if (plot_scores): plot_evolve(params, samples, i, labels)\n",
        "    if ( (i > 0) and (verbose==True) ): \n",
        "      print(f'loss at step {i}: {loss_vector[i]} loss at prev step {loss_vector[i-1]}')\n",
        "  print(f'initial loss: {loss_vector[0]}')\n",
        "  print(f'final loss: {loss_vector[-1]}')\n",
        "\n",
        "\n",
        "if plot_loss:\n",
        "  fig , ax = plt.subplots(1,1,figsize=(12, 8), facecolor='white',dpi = 70)\n",
        "  steps = range(0,n_epochs)\n",
        "  plt.plot(steps,loss_vector, alpha = 0.80, zorder=0)\n",
        "  plt.scatter(steps,loss_vector, s=20, zorder=1)\n",
        "  plt.xlabel('training epochs', fontsize = 30)\n",
        "  plt.ylabel('cross-entropy loss (arb)', fontsize = 30)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('loss_evolution.png',facecolor='white',dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXfpd5tptAOt",
        "outputId": "72601a87-db35-4a44-91d6-64744b9fc1fb"
      },
      "outputs": [],
      "source": [
        "# ------------------------- #\n",
        "# langevin dynamic sampling #\n",
        "# ------------------------- #\n",
        "def anneal_Langevin_dynamics(x_mod, scorenet, params, sigmas, rng, n_steps_each=100, \n",
        "                                step_lr=0.000008,denoise=True):\n",
        "    # initialise arrays for images and scores\n",
        "    images = []\n",
        "    scores = []\n",
        "\n",
        "    # loop over noise levels from high to low for sample generation\n",
        "    for c, sigma in enumerate(sigmas):\n",
        "        labels = jax.numpy.ones(x_mod.shape[0],dtype=np.int8) * c\n",
        "        step_size = step_lr * (sigma / sigmas[-1]) ** 2\n",
        "        desc = 'sampling at noise level: ' + str(c + 1) + ' / ' + str(len(sigmas))\n",
        "        for s in tqdm(range(n_steps_each),desc=desc):\n",
        "            grad = scorenet.apply({'params' : params}, x_mod, labels)\n",
        "            noise = jax.random.normal(rng, shape=x_mod.shape)\n",
        "            x_mod = x_mod + step_size * grad + noise * jax.numpy.sqrt(step_size * 2)\n",
        "            images.append(x_mod[0].squeeze())\n",
        "            scores.append(grad[0].squeeze())\n",
        "\n",
        "    # add a final denoising step is desired\n",
        "    if denoise:\n",
        "        last_noise = (len(sigmas) - 1) * jax.numpy.ones(x_mod.shape[0], dtype=np.int8)\n",
        "        last_grad = scorenet.apply({'params' : params}, x_mod, last_noise)\n",
        "        x_mod = x_mod + sigmas[-1] ** 2 * last_grad\n",
        "        images.append(x_mod[0].squeeze())\n",
        "        scores.append(last_grad[0].squeeze())\n",
        "\n",
        "    return images, scores\n",
        "\n",
        "# TODO: JIT compile the function for serious speed ups in the for loop\n",
        "# think will need static model args - use functool.partial(XXXX)?\n",
        "#anneal_Langevin_dynamics = jax.jit(anneal_Langevin_dynamics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------- #\n",
        "# testing sampling #\n",
        "# ---------------- #\n",
        "n_samples      = 1                               # number of samples to generate\n",
        "sample_steps   = 30                              # number of steps to take at each noise level\n",
        "shape_array    = jnp.array(range(0, n_samples))  # run Langevin dynamics on n_samples\n",
        "data_shape     = data_jax[shape_array]           # get the data shape for starting image\n",
        "gaussian_noise = jax.random.normal(key_seq, shape=data_shape.shape) # Initial noise image/data\n",
        "\n",
        "# run the Langevin sampler\n",
        "images, scores = anneal_Langevin_dynamics(  gaussian_noise, \n",
        "                                            model, \n",
        "                                            best_params, \n",
        "                                            sigmas, \n",
        "                                            key_seq,\n",
        "                                            n_steps_each=sample_steps, \n",
        "                                            denoise=True  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------- #\n",
        "# plot sample evolution with generation steps #\n",
        "# ------------------------------------------- #\n",
        "# NOTE: for single sample data, easy to adjust if running multiple samples\n",
        "\n",
        "images_array = np.array(images)\n",
        "col_map = cmr.lilac\n",
        "fig , ax = plt.subplots(2,5,figsize=(16, 7), facecolor='white',dpi = 70)\n",
        "plt_idx = int( len(images_array) / 10 )\n",
        "n_panels = 7\n",
        "step_array =  range(0, sample_steps, int(sample_steps / (n_panels - 1)) )\n",
        "for i in range(n_panels):\n",
        "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "    plt.subplot(1,n_panels,i + 1)\n",
        "    if (i < n_panels - 1):\n",
        "        step = step_array[i] * num_scales # note num_scales factor is for the number of noise levels\n",
        "        name = 'step ' + str(int( step / num_scales ))\n",
        "        plt.title(name, fontsize = 24)\n",
        "        plt.imshow(images_array[step], cmap=col_map)\n",
        "        plt.axis('off')\n",
        "    else: \n",
        "        name = 'final step ' + str(int( sample_steps ))\n",
        "        plt.title(name, fontsize = 24)\n",
        "        plt.imshow(images_array[-1], cmap=col_map)\n",
        "        plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('langevin_sampling_panels.png',facecolor='white',dpi=300)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "vscode": {
      "interpreter": {
        "hash": "3bc68688a3ed8a5ce6018ce4ac9bdddfcbacccfa8757535ca0e210f8a40fad74"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
